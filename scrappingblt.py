# -*- coding: utf-8 -*-
"""scrappingBLT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jA57EDvFrzD-wtz_7ccTbBMBE_oMwrmw
"""

import requests
from bs4 import BeautifulSoup
import time
import csv
from urllib.parse import urljoin # Import di awal
import random
import traceback
import re # <--- PASTIKAN IMPORT INI ADA

# URL awal / halaman pertama (dari contoh Anda)
current_page_url = 'https://www.detik.com/search/searchall?query=blt&page=1&result_type=relevansi'

# Untuk menyimpan semua data yang di-scrape
semua_data_scraped = []

# Opsional: Batas maksimal data yang ingin diambil
MAKSIMAL_DATA_TOTAL = 1000 # Ubah sesuai kebutuhan, atau None jika ingin semua
jumlah_data_terscrape_total = 0

# Opsional: Batas maksimal halaman yang ingin di-scrape
MAKSIMAL_HALAMAN = None # Diatur None agar mencoba semua halaman (atau sampai MAKSIMAL_DATA_TOTAL)
jumlah_halaman_terscrape = 0

# Header, termasuk User-Agent
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

print("Memulai proses scraping...")

while current_page_url and \
      (MAKSIMAL_HALAMAN is None or jumlah_halaman_terscrape < MAKSIMAL_HALAMAN) and \
      (MAKSIMAL_DATA_TOTAL is None or jumlah_data_terscrape_total < MAKSIMAL_DATA_TOTAL):

    print(f"Mengambil data dari halaman: {current_page_url}")
    jumlah_halaman_terscrape += 1

    try:
        response = requests.get(current_page_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # --- BAGIAN UNTUK SCRAPE DATA DARI HALAMAN SAAT INI ---
        # Selector ini (soup.find_all('article')) sepertinya sudah bekerja baik untuk Anda
        items_di_halaman_ini = soup.find_all('article')

        if not items_di_halaman_ini and jumlah_halaman_terscrape > 1:
            print("Tidak ada item ditemukan di halaman ini, mungkin halaman terakhir atau selector item salah.")
            break
        elif not items_di_halaman_ini and jumlah_halaman_terscrape == 1:
            print("Tidak ada item ditemukan di halaman PERTAMA. Periksa selector item ('article') Anda dengan sangat teliti!")
            break

        print(f"Ditemukan {len(items_di_halaman_ini)} item di halaman ini.")

        for item_index, item in enumerate(items_di_halaman_ini):
            if MAKSIMAL_DATA_TOTAL is not None and jumlah_data_terscrape_total >= MAKSIMAL_DATA_TOTAL:
                print(f"Batas maksimal {MAKSIMAL_DATA_TOTAL} data telah tercapai.")
                current_page_url = None
                break

            try:
                # Selector judul Anda (item.find('h3', class_='media__title')) sepertinya sudah bekerja
                title_element = item.find('h3', class_='media__title')
                judul_berita = title_element.get_text(strip=True) if title_element else "Judul tidak ditemukan"

                # Selector link Anda (item.find('a')) mungkin perlu diperhatikan agar selalu mengambil link utama berita
                # Jika item <article> itu sendiri adalah link, Anda bisa: link_element = item (jika item adalah tag 'a')
                # atau link_element = item.find('a', ...) yang lebih spesifik.
                # Untuk sekarang, kita biarkan item.find('a') jika ini bekerja untuk Anda.
                link_element = item.find('a')
                link_berita = link_element['href'] if link_element and link_element.has_attr('href') else "Link tidak ditemukan"

                if link_berita != "Link tidak ditemukan" and not link_berita.startswith('http'):
                    link_berita = urljoin(response.url, link_berita)

                data_item = {'judul': judul_berita, 'link': link_berita, 'halaman_sumber': current_page_url}
                semua_data_scraped.append(data_item)
                jumlah_data_terscrape_total += 1
                print(f"  - Data ke-{jumlah_data_terscrape_total}: {judul_berita} ({link_berita})")
            except Exception as e_item:
                print(f"  Error saat memproses satu item ke-{item_index+1}: {e_item}")

        if current_page_url is None: # Jika sudah di-set None oleh inner loop (batas data tercapai)
            break

        # --- BAGIAN UNTUK MENEMUKAN HALAMAN BERIKUTNYA (DIPERBAIKI) ---
        next_button = None # Inisialisasi

        # Upaya 1: Cari berdasarkan atribut rel="next"
        # Ini seringkali cara paling standar dan andal jika situs menggunakannya.
        # Periksa apakah tombol 'Next' di Detik memiliki atribut rel="next"
        if not next_button:
            next_button = soup.find('a', rel='next')
            if next_button:
                print("Tombol 'Next' ditemukan via rel='next'")

        # Upaya 2: Cari berdasarkan teks "Berikutnya" atau "Next" (case-insensitive)
        # Ini efektif jika teks tombolnya konsisten.
        if not next_button:
            # Mencari di semua link yang mungkin adalah bagian dari pagination
            # Anda mungkin perlu menyesuaikan class_ di sini jika 'pagination__item' terlalu umum
            # atau jika tombol 'Next' tidak memiliki class pagination__item
            possible_buttons = soup.find_all('a', class_=re.compile(r'pagination|page-link|next', re.IGNORECASE)) # Mencari class yang umum untuk pagination
            if not possible_buttons: # Jika tidak ada class umum, cari semua link saja (kurang efisien)
                possible_buttons = soup.find_all('a')

            for button in possible_buttons:
                button_text = button.get_text(strip=True).lower()
                # Cek teks yang sangat umum untuk "Next"
                if button_text == "berikutnya" or button_text == "next" or button_text == "selanjutnya" or button_text == "Â»":
                    next_button = button
                    print(f"Tombol 'Next' ditemukan via teks persis: '{button_text}'")
                    break
            # Jika teks persis tidak ada, coba yang mengandungnya (lebih longgar)
            if not next_button:
                 for button in possible_buttons:
                    button_text = button.get_text(strip=True).lower()
                    if "berikutnya" in button_text or "next" in button_text or "selanjutnya" in button_text:
                        next_button = button
                        print(f"Tombol 'Next' ditemukan via teks mengandung: '{button_text}'")
                        break

        # Upaya 3: Cari berdasarkan atribut title atau aria-label (jika ada)
        # Seringkali tombol memiliki tooltip atau label aksesibilitas.
        if not next_button:
            # title*="Berikutnya" berarti title mengandung kata "Berikutnya"
            next_button = soup.select_one('a[title*="Berikutnya"], a[title*="Next"], a[aria-label*="Berikutnya"], a[aria-label*="Next"]')
            if next_button:
                print("Tombol 'Next' ditemukan via atribut title atau aria-label")

        # --- AKHIR UPAYA PENCARIAN TOMBOL NEXT ---

        if next_button and next_button.has_attr('href'):
            next_page_link = next_button['href']
            # Pastikan URL adalah absolut
            current_page_url = urljoin(response.url, next_page_link)
            print(f"Link halaman berikutnya ditemukan: {current_page_url}")
        else:
            print("Tombol 'Next' (Berikutnya) TIDAK ditemukan dengan metode spesifik. Mengakhiri pagination.")
            current_page_url = None # Hentikan loop
        # --- AKHIR BAGIAN MENEMUKAN HALAMAN BERIKUTNYA ---

        # Beri jeda agar tidak membebani server
        if current_page_url:
            jeda = random.uniform(3, 7) # Jeda acak antara 3-7 detik
            print(f"Menunggu {jeda:.2f} detik sebelum ke halaman berikutnya...")
            time.sleep(jeda)

    except requests.exceptions.HTTPError as e:
        print(f"Error HTTP saat mengakses {current_page_url if isinstance(current_page_url, str) else 'URL tidak valid'}: {e}")
        if hasattr(e, 'response') and e.response is not None and e.response.status_code == 404:
            print("Halaman tidak ditemukan (404), mungkin halaman terakhir yang valid.")
        break
    except requests.exceptions.RequestException as e:
        print(f"Error koneksi saat mengakses {current_page_url if isinstance(current_page_url, str) else 'URL tidak valid'}: {e}")
        break
    except Exception as e:
        print(f"Terjadi kesalahan umum: {e}")
        traceback.print_exc() # Cetak traceback untuk debug lebih detail
        break

print(f"\nProses scraping selesai.")
print(f"Total data yang berhasil di-scrape dari {jumlah_halaman_terscrape} halaman: {len(semua_data_scraped)} item.")

# Menyimpan ke CSV
if semua_data_scraped:
    nama_file_csv = 'data_detik_blt.csv'
    if semua_data_scraped:
        fieldnames = semua_data_scraped[0].keys()
        try:
            with open(nama_file_csv, mode='w', newline='', encoding='utf-8') as file_csv:
                writer = csv.DictWriter(file_csv, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(semua_data_scraped)
            print(f"Data telah disimpan ke {nama_file_csv}")
        except IOError:
            print(f"Error: Tidak bisa menulis ke file {nama_file_csv}")
        except Exception as e:
            print(f"Error saat menyimpan ke CSV: {e}")
else:
    print("Tidak ada data untuk disimpan.")